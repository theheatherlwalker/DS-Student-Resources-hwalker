setwd("~/Documents/GitHub/DS-Student-Resources/DS104-Data-Wrangling-and-Visualization/Assignments/Heather Walker - DS104-04-19 - Lesson 4 Hands-On")
setwd("~/Documents/GitHub/DS-Student-Resources/DS106-Machine-Learning/Modeling/Assignments/Heather Walker - DS106-02-08 - Lesson 2 Hands-On - Minerals")
# CALL PACKAGES / LIBRARIES
# to conduct logistic regression in R:
# to test assumptions
library("caret")
library("lmtest")
# for data wrangling
library("magrittr")
library("dplyr")
library("tidyr")
library("e1071")
# to graph logistic regression model
library("popbio")
minerals = read.csv("data/minerals.csv")
View(minerals)
mylogit <- glm(WinsR ~ HR.Count, data=minerals, family="binomial")
mylogit <- glm(Gold ~ Antimony, data=minerals, family="binomial")
probabilities <- predict(mylogit, type = "response")
probabilities <- predict(mylogit, type = "response")
minerals$Predicted <- ifelse(probabilities > .5, "pos", "neg")
minerals$PredictedR <- NA
minerals$PredictedR[minerals$Predicted=='pos'] <- 1
minerals$PredictedR[minerals$Predicted=='neg'] <- 0
minerals$PredictedR <- as.factor(minerals$PredictedR)
minerals$Gold <- as.factor(minerals$Gold)
conf_mat <- caret::confusionMatrix(minerals$PredictedR, minerals$Gold)
conf_mat
minerals1 <- minerals %>%
dplyr::select_if(is.numeric)
predictors <- colnames(minerals1)
minerals1 <- minerals1 %>%
mutate(logit=log(probabilities/(1-probabilities))) %>%
gather(key= "predictors", value="predictor.value", -logit)
ggplot(minerals1, aes(logit, predictor.value))+
geom_point(size=.5, alpha=.5)+
geom_smooth(method= "loess")+
theme_bw()+
facet_wrap(~predictors, scales="free_y")
plot(mylogit$residuals)
# meets the assumption of independent errors!
dwtest(mylogit, alternative="two.sided")
#If this test is not statistically significant (> .05),
# then you are automatically good to go,
# and you have independent errors.
infl <- influence.measures(mylogit)
summary(infl)
summary(mylogit)
# TEST FOR ERRORS Option 1:
# Graph the errors (residuals)
logi.hist.plot(minerals$Antimony.Count,minerals$Gold,
boxp=FALSE, type="hist", col="gray")
logi.hist.plot(minerals$Antimony,minerals$Gold,
boxp=FALSE, type="hist", col="gray")
setwd("~/Documents/GitHub/DS-Student-Resources/DS104-Data-Wrangling-and-Visualization/Assignments/Heather Walker - DS104-04-19 - Lesson 4 Hands-On")
setwd("~/Documents/GitHub/DS-Student-Resources/DS106-Machine-Learning/Modeling/Assignments/Heather Walker - DS106-03-07 - Lesson 3 Hands-On - Non-Linear Regression/code")
setwd("~/Documents/GitHub/DS-Student-Resources/DS104-Data-Wrangling-and-Visualization/Assignments/Heather Walker - DS104-04-19 - Lesson 4 Hands-On")
setwd("~/Documents/GitHub/DS-Student-Resources/DS106-Machine-Learning/Modeling/Assignments/Heather Walker - DS106-03-07 - Lesson 3 Hands-On - Non-Linear Regression")
nonlinear = read.csv("data/nonlinear.csv")
setwd("~/Documents/GitHub/DS-Student-Resources/DS106-Machine-Learning/Modeling/Assignments/Heather Walker - DS106-03-07 - Lesson 3 Hands-On - Non-Linear Regression")
nonlinear = read.csv("data/nonlinear.csv")
View(nonlinear)
library("ggplot2")
setwd("~/Documents/GitHub/DS-Student-Resources/DS106-Machine-Learning/Modeling/Assignments/Heather Walker - DS106-03 - Examples")
library("ggplot2")
bluegill_fish = read.csv("data/bluegill_fish.csv")
View(bluegill_fish)
quadPlot <- ggplot(bluegill_fish, aes(x = age, y=length)) +
geom_point() + stat_smooth(method = "lm",
formula = y ~x + I(x^2), size =1)
quadPlot
Agesq <- bluegill_fish$age^2
quadModel <- lm(bluegill_fish$length~bluegill_fish$age+Agesq)
summary(quadModel)
bacteria = read.csv("data/bacteria.csv")
View(bacteria)
exMod <- lm(log(bacteria$Count)~bacteria$Period)
summary(exMod)
library("ggplot2")
nonlinear = read.csv("data/nonlinear.csv")
setwd("~/Documents/GitHub/DS-Student-Resources/DS106-Machine-Learning/Modeling/Assignments/Heather Walker - DS106-03-07 - Lesson 3 Hands-On - Non-Linear Regression")
nonlinear = read.csv("data/nonlinear.csv")
View(nonlinear)
quadPlot <- ggplot(nonlinear, aes(x = age, y=length)) +
geom_point() + stat_smooth(method = "lm",
formula = y ~x + I(x^2), size =1)
quadPlot
quadPlot <- ggplot(nonlinear, aes(x = X1, y=Y1)) +
geom_point() + stat_smooth(method = "lm",
formula = y ~x + I(x^2), size =1)
quadPlot
quadPlot <- ggplot(nonlinear, aes(x = X1, y=Y1)) +
ggplot(nonlinear, aes(x=X2, y=Y2))
library("ggplot2")
# Libraries suggested from an example
library(dplyr)
library(patchwork) # To display 2 charts together
library("ggplot2")
# Libraries suggested from an example
library(dplyr)
install.packages(patchwork)
library(patchwork) # To display 2 charts together
install.packages(patchwork)
install.packages(patchwork)
install.packages("patchwork")
library(patchwork) # To display 2 charts together
install.packages("hrbrthemes")
library(hrbrthemes)
nonlinearSet1 <- ggplot(nonlinear, aes(x = X1, y=Y1)) +
geom_point() +
stat_smooth(method = "lm", formula = y ~x + I(x^2), size =1) +
theme_ipsum()
nonlinearSet2 <- ggplot(nonlinear, aes(x = X2, y=Y2)) +
geom_point() +
stat_smooth(method = "lm", formula = y ~x + I(x^2), size =1) +
theme_ipsum()
nonlinearSet1 + nonlinearSet2
nonlinearBoth <- ggplot() +
geom_point(data = nonlinearSet1, aes(x = X1, y=Y1), color = "blue") +
geom_point(data = nonlinearSet2, aes(x = X2, y=Y2), color = "red") +
stat_smooth(method = "lm", formula = y ~x + I(x^2), size =1) +
theme_ipsum()
nonlinearBoth <- ggplot() +
geom_point(data = nonlinear, aes(x = X1, y=Y1), color = "blue") +
geom_point(data = nonlinear, aes(x = X2, y=Y2), color = "red") +
stat_smooth(method = "lm", formula = y ~x + I(x^2), size =1) +
theme_ipsum()
nonlinearBoth
nonlinearBoth2 <- ggplot() +
geom_point(data = nonlinear, aes(X1, Y1), color = "blue") +
geom_point(data = nonlinear, aes(X2, Y2), color = "red") +
stat_smooth(method = "lm", formula = Y1 ~ X1 + I(X1^2), size =1) +
stat_smooth(method = "lm", formula = Y2 ~ X2 + I(X1^2), size =1) +
theme_ipsum()
nonlinearBoth2
nonlinearBoth2 <- ggplot() +
geom_point(data = nonlinear, aes(X1, Y1), color = "blue") +
geom_point(data = nonlinear, aes(X2, Y2), color = "red") +
theme_ipsum()
nonlinearBoth2
nonlinearBoth2 <- ggplot(nonlinear, aes(X1, Y1, colour = factor(X2), shape = factor(Y2) )) + geom_point()
nonlinearBoth2
nonlinearBoth2 <- ggplot(nonlinear, aes(X1, Y1, colour = X2, shape = Y2 )) + geom_point()
nonlinearBoth2
nonlinearBoth+ labs( "blue" = "X1, Y1",
"red" = "X2, Y2")
nonlinearBoth <- ggplot() +
geom_point(data = nonlinear, aes(x = X1, y=Y1), color = "blue") +
geom_point(data = nonlinear, aes(x = X2, y=Y2), color = "red") +
nonlinearBoth+ labs( "blue" = "X1, Y1",
"red" = "X2, Y2")
nonlinearBoth <- ggplot() +
geom_point(data = nonlinear, aes(x = X1, y=Y1), color = "yellow") +
geom_point(data = nonlinear, aes(x = X2, y=Y2), color = "red") +
nonlinearBoth+ labs( "blue" = "X1, Y1",
"red" = "X2, Y2")
nonlinearBoth + labs( "blue" = "X1, Y1",
"red" = "X2, Y2")
nonlinearBoth <- ggplot() +
geom_point(data = nonlinear, aes(x = X1, y=Y1), color = "blue") +
geom_point(data = nonlinear, aes(x = X2, y=Y2), color = "red") +
nonlinearBoth
# must include argument label "data"
nonlinearBoth <- ggplot() +
geom_point(data = nonlinear, aes(x = X1, y=Y1), color = "blue") +
geom_point(data = nonlinear, aes(x = X2, y=Y2), color = "red") +
# DISPLAY PLOT
nonlinearBoth
nonlinearBoth
View(nonlinearBoth)
nonlinearBoth1 <- ggplot() +
geom_point(data = nonlinear, aes(x = X1, y=Y1), color = "blue") +
geom_point(data = nonlinear, aes(x = X2, y=Y2), color = "red") +
# must include argument label "data"
nonlinearBoth1 <- ggplot() +
geom_point(data = nonlinear, aes(x = X1, y=Y1), color = "blue") +
geom_point(data = nonlinear, aes(x = X2, y=Y2), color = "red") +
# DISPLAY PLOT
nonlinearBoth1
nonlinearBoth1 <- ggplot() +
geom_point(data = nonlinear, aes(x = X1, y=Y1), color = "blue") +
geom_point(data = nonlinear, aes(x = X2, y=Y2), color = "red") +
nonlinearBoth1
nonlinearBoth1 <- ggplot() +
geom_point(data = nonlinear, aes(x = X1, y=Y1), color = "blue") +
geom_point(data = nonlinear, aes(x = X2, y=Y2), color = "red")
# DISPLAY PLOT
nonlinearBoth1
nonlinearSet1Graph <- ggplot(nonlinear, aes(x = X1, y=Y1)) +
geom_point() +
stat_smooth(method = "lm", formula = y ~x + I(x^2), size =1) +
theme_ipsum()
nonlinearBoth1 + labs(red = "X2, Y2")
nonlinearBoth1 + labs("red" = "X2, Y2")
nonlinearSet1Graph <- ggplot(nonlinear, aes(x = X1, y=Y1)) +
geom_point() +
stat_smooth(method = "lm", formula = y ~x + I(x^2), size =1) +
theme_ipsum()
nonlinearSet2Graph <- ggplot(nonlinear, aes(x = X2, y=Y2)) +
geom_point() +
stat_smooth(method = "lm", formula = y ~x + I(x^2), size =1) +
theme_ipsum()
nonlinearSet1 + nonlinearSet2
X1squared <- nonlinear$X1^2
X2squared <- nonlinear$X2^2
X1quadModel <- lm(nonlinear$Y1~nonlinear$X1+X1squared)
X2quadModel <- lm(nonlinear$Y2~nonlinear$X2+X2squared)
summary(X1quadModel)
summary(X2quadModel)
plot(cars)
```{r}
```{r}
head(mtcars)
head(mtcars)
# Create a linear model of all 10 predictor variables.
FitAll = lm(mpg ~ ., data = mtcars)
# Create a linear model of all 10 predictor variables.
FitAll = lm(mpg ~ ., data = mtcars)
summary(FitAll)
step(FitAll, direction = 'backward')
# Create new model with just the un-eliminated variables:
fitsome = lm(mpg ~ am + qsec + wt, data = mtcars)
fitsome = lm(mpg ~ am + qsec + wt, data = mtcars)
# Create new model with just the un-eliminated variables:
fitsome = lm(mpg ~ am + qsec + wt, data = mtcars)
# Create new model with just the un-eliminated variables:
fitsome = lm(mpg ~ am + qsec + wt, data = mtcars)
# Create new model with just the un-eliminated variables:
fitsome = lm(mpg ~ am + qsec + wt, data = mtcars)
# Create new model with just the un-eliminated variables:
FitSome = lm(mpg ~ am + qsec + wt, data = mtcars)
# Create new model with just the un-eliminated variables:
fitsome = lm(mpg ~ am + qsec + wt, data = mtcars)
fitsome
# Create new model with just the un-eliminated variables:
fitsome = lm(mpg ~ am + qsec + wt, data = mtcars)
# Summary of model
summary(FitSome)
head(mtcars)
# Create a linear model of all 10 predictor variables.
fitstart = lm(mpg ~ 1, data = mtcars)
head(mtcars)
# QUESTION:
# Create a model that will use `mpg` as the response variable, and the other 10 columns of data as potential predictor variables. It is assumed that all 10 predictors don't really belong in the model.
# Summary of model
summary(fitstart)
# Create a complete forward selection
step(fitstart, direction = 'forward', scope = (~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb))
# Create a model with only the identified variables
fitsome2 = lm(mpg ~ wt + cyl + hp, data = mtcars)
# Create a model with only the identified variables
fitsome2 = lm(mpg ~ wt + cyl + hp, data = mtcars)
summary(fitsome2)
step(fitstart, direction="both", scope=formula(FitAll))
setwd("~/Documents/GitHub/DS-Student-Resources/DS106-Machine-Learning/Modeling/Assignments/Heather Walker - DS106-04-08 - Lesson 4 Practice Hands-On - Stepwise Regression")
iq = read.csv("data/IQ.csv")
iq = read.csv("data/IQ.csv")
iq = read.csv("data/IQ.csv")
head(iq)
# QUESTIONS:
# Which model is the best? Why?
# From the best model, what is the adjusted R2 value and what does it mean?
# From the best model, how does each variable influence IQ?
# Create a linear model of all 10 predictor variables.
FitAll = lm(IQ ~ ., data = iq)
# Summary of model
summary(FitAll)
step(FitAll, direction = 'backward')
# Create new model with just the un-eliminated variables:
fitsome = lm(IQ ~ Test1 + Test2 + Test4, data = mtcars)
# Create new model with just the un-eliminated variables:
fitsome = lm(iq ~ Test1 + Test2 + Test4, data = mtcars)
# Create new model with just the un-eliminated variables:
fitsome = lm(formula = IQ ~ Test1 + Test2 + Test4, data = iq)
# Summary of model
summary(FitSome)
# Create new model with just the un-eliminated variables:
fitsome = lm(formula = IQ ~ Test1 + Test2 + Test4, data = iq)
# Summary of model
summary(FitSome)
# Create new model with just the un-eliminated variables:
fitsome = lm(formula = IQ ~ Test1 + Test2 + Test4, data = iq)
# Summary of model
summary(FitSome)
# Create new model with just the un-eliminated variables:
FitSome = lm(formula = IQ ~ Test1 + Test2 + Test4, data = iq)
# Summary of model
summary(FitSome)
iq = read.csv("data/IQ.csv")
head(iq)
html_d
title: "DS106-04-08 - Lesson 4 Practice Hands-On - Stepwise Regression - Part 2"
# A: Backwards Elimination
# Create a linear model of all predictor variables.
FitAll = lm(IQ ~ ., data = iq)
# Summary of model
summary(FitAll)
iq = read.csv("data/IQ.csv")
head(iq)
stepwiseRegression = read.csv("data/stepwiseRegression.csv")
head(stepwiseRegression)
# A: Backwards Elimination
# Create a linear model of all predictor variables.
FitAll = lm(Y ~ ., data = stepwiseRegression)
# Summary of model
summary(FitAll)
step(FitAll, direction = 'backward')
# FORWARD SELECTION
# Create a linear model of all predictor variables.
FitStart = lm(Y ~ 1, data = stepwiseRegression)
# Summary of model
summary(FitStart)
step(FitStart, direction = 'forward')
# HYBRID STEPWISE
# Create model
step(FitStart, direction = 'both')
# FORWARD SELECTION
# Create a linear model of all predictor variables.
FitStart = lm(Y ~ 1, data = stepwiseRegression)
# Summary of model
summary(FitStart)
step(FitStart, direction = 'forward', scope = (formula(FitAll)))
# HYBRID STEPWISE
# Create model
step(FitStart, direction = 'both', scope = (formula(FitAll)))
trials <- 10000
# Priors you created = 90% effective
alpha <- 9
beta <- 1
# *Bleach Alone*: mold returned 27%, did not return 39%
samplesA <- rbeta(trials, 27 + alpha, 39 + beta)
# *Bleach + Scrubbing*: mold returned 10%, did not return 45%
samplesB <- rbeta(trials, 73 + alpha, 45 + beta)
Bsuperior <- sum(samplesB > samplesA) / trials
# print results
Bsuperior
trials <- 10000
# Priors you created = 90% effective
alpha <- 9
beta <- 1
# *Bleach Alone*: mold returned 27%, did not return 39%
samplesA <- rbeta(trials, 27 + alpha, 39 + beta)
# *Bleach + Scrubbing*: mold returned 10%, did not return 45%
samplesB <- rbeta(trials, 10 + alpha, 45 + beta)
Bsuperior <- sum(samplesB > samplesA) / trials
# print results
Bsuperior
setwd("~/Documents/GitHub/DS-Student-Resources/DS103-Metrics-and-Data-Processing/Assignments/Heather Walker - DS103-05-12 - Lesson 5 Hands-On")
library("corpcor")
library(corpcor)
library("corpcor-package")
install.packages("corpcor-package")
financialWB = read.csv("data/financialWB.csv")
setwd("~/Documents/GitHub/DS-Student-Resources/DS103-Metrics-and-Data-Processing/Assignments/Heather Walker - DS103-05-12 - Lesson 5 Hands-On")
financialWB = read.csv("data/financialWB.csv")
library("corpcor-package")
install.packages("corpcor-package")
install.packages("corpcor-package")
install.packages("corpcor")
library("corpcor-package")
library("corpcor")
library("GPArotation")
library("psych")
library("IDPmisc")
financialWB = read.csv("data/financialWB.csv")
setwd("~/Documents/GitHub/DS-Student-Resources/DS103-Metrics-and-Data-Processing/Assignments/Heather Walker - DS103-05 - examples")
financialWB = read.csv("data/financialWB.csv")
View(financialWB)
financialWB1 <- financialWB[, 8:17]
financialWBmatrix <- cor(financialWB1)
View(round(financialWBmatrix, 2)) # view the matrix
# ----------------------------------------------------
#  | |  Barlett's Test (confirm correlation matrix findings)
# ----------------------------------------------------
cortest.bartlett(financialWB1)
# ----------------------------------------------------
#  | |  Check your Determinants (confirm correlation matrix findings)
# ----------------------------------------------------
det(financialWBmatrix)
pcModel1 <- principal(financialWB1, nfactors = 10, rotate = "none")
pcModel1
# ----------------------------------------------------
#  | Examine the Scree Plot
# ----------------------------------------------------
plot(pcModel1$values, type="b")
plot(pcModel1$values, type="b")
pcModel2 <- principal(financialWB1, nfactors = 2, rotate = "none")
pcModel2
residuals <- factor.residuals(financialWBmatrix, pcModel2$loadings)
# Second line: formats residuals as matrix and keeps only top half
residuals <- as.matrix(residuals[upper.tri(residuals)])
# This line will find only the large residuals values and put in variable
largeResid <- abs(residuals) > .05
# find the number of residuals that are large
sum(largeResid)
# get the percentage of residuals that are large as
# compared to the total number of rows
sum(largeResid/nrow(residuals))
pcModel3 <- principal(financialWB1, nfactors = 2, rotate = "oblimin")
pcModel3
# The line of code below will print out only the loadings
# that are higher than .3, and sorts them from largest to
# smallest.
print.psych(pcModel3, cut = .3, sort=TRUE)
print.psych(pcModel3, cut = .3, sort=TRUE)
pcModel4 <- principal(financialWB1, nfactors = 2, rotate = "varimax")
print.psych(pcModel4, cut=.3, sort=TRUE)
library("psych")
library("psych")
financialWB = read.csv("data/financialWB.csv")
View(financialWB)
goodFWB <- financialWB1[, c(1,2,4,8)]
badFWB <- financialWB1[, c(3,5,6,7,9,10)]
financialWB1$FWB1_3r <- NA
financialWB1$FWB1_3r[financialWB1$FWB1_3 == 1] <- 5
financialWB1$FWB1_3r[financialWB1$FWB1_3 == 2] <- 4
financialWB1$FWB1_3r[financialWB1$FWB1_3 == 3] <- 3
financialWB1$FWB1_3r[financialWB1$FWB1_3 == 4] <- 2
financialWB1$FWB1_3r[financialWB1$FWB1_3 == 5] <- 1
financialWB1$FWB1_5r <- NA
financialWB1$FWB1_5r[financialWB1$FWB1_5 == 1] <- 5
financialWB1$FWB1_5r[financialWB1$FWB1_5 == 2] <- 4
financialWB1$FWB1_5r[financialWB1$FWB1_5 == 3] <- 3
financialWB1$FWB1_5r[financialWB1$FWB1_5 == 4] <- 2
financialWB1$FWB1_5r[financialWB1$FWB1_5 == 5] <- 1
financialWB1$FWB1_6r <- NA
financialWB1$FWB1_6r[financialWB1$FWB1_6 == 1] <- 5
financialWB1$FWB1_6r[financialWB1$FWB1_6 == 2] <- 4
financialWB1$FWB1_6r[financialWB1$FWB1_6 == 3] <- 3
financialWB1$FWB1_6r[financialWB1$FWB1_6 == 4] <- 2
financialWB1$FWB1_6r[financialWB1$FWB1_6 == 5] <- 1
financialWB1$FWB2_1r <- NA
financialWB1$FWB2_1r[financialWB1$FWB2_1 == 1] <- 5
financialWB1$FWB2_1r[financialWB1$FWB2_1 == 2] <- 4
financialWB1$FWB2_1r[financialWB1$FWB2_1 == 3] <- 3
financialWB1$FWB2_1r[financialWB1$FWB2_1 == 4] <- 2
financialWB1$FWB2_1r[financialWB1$FWB2_1 == 5] <- 1
financialWB1$FWB2_3r <- NA
financialWB1$FWB2_3r[financialWB1$FWB2_3 == 1] <- 5
financialWB1$FWB2_3r[financialWB1$FWB2_3 == 2] <- 4
financialWB1$FWB2_3r[financialWB1$FWB2_3 == 3] <- 3
financialWB1$FWB2_3r[financialWB1$FWB2_3 == 4] <- 2
financialWB1$FWB2_3r[financialWB1$FWB2_3 == 5] <- 1
financialWB1$FWB2_4r <- NA
financialWB1$FWB2_4r[financialWB1$FWB2_4 == 1] <- 5
financialWB1$FWB2_4r[financialWB1$FWB2_4 == 2] <- 4
financialWB1$FWB2_4r[financialWB1$FWB2_4 == 3] <- 3
financialWB1$FWB2_4r[financialWB1$FWB2_4 == 4] <- 2
financialWB1$FWB2_4r[financialWB1$FWB2_4 == 5] <- 1
financialWB2 <- financialWB1[, c(1,2,4,8,11,12,13,14,15,16)]
alpha(goodFWB)
alpha(badFWB)
alpha(financialWB2)
setwd("~/Documents/GitHub/DS-Student-Resources/DS104-Data-Wrangling-and-Visualization/Assignments/Heather Walker - DS104-04-19 - Lesson 4 Hands-On")
setwd("~/Documents/GitHub/DS-Student-Resources/DS103-Metrics-and-Data-Processing/Assignments/Heather Walker - DS103-05-12 - Lesson 5 Hands-On")
library("corpcor")
library("GPArotation")
library("psych")
library("IDPmisc")
studentSurvey = read.csv("data/studentSurvey.csv")
setwd("~/Documents/GitHub/DS-Student-Resources/DS103-Metrics-and-Data-Processing/Assignments/Heather Walker - DS103-05-12 - Lesson 5 Hands-On")
studentSurvey = read.csv("data/studentSurvey.csv")
studentSurvey = read.csv("data/studentSurvey.csv")
studentSurvey = read.csv("data/studentSurvey.csv")
View(studentSurvey)
View(financialWB)
studentSurvey1 <- studentSurvey[, 31:42]
studentSurveyMatrix <- cor(studentSurvey1) # create correlation matrix
View(round(studentSurveyMatrix, 2)) # view the matrix
studentSurveyMatrix
studentSurvey1
library(dplyr)
library(tidyr)
studentSurvey1edit <- drop_na(studentSurvey1, ...)
studentSurvey1edit <- drop_na(studentSurvey1, NA)
studentSurvey1edit <- drop_na(studentSurvey1, any_of(vars))
studentSurvey1edit %>% drop_na()
studentSurvey1 %>% drop_na()
studentSurvey1
studentSurvey1edit <- drop_na(studentSurvey1)
View(studentSurvey1edit)
studentSurveyMatrix <- cor(studentSurvey1edit) # create correlation matrix
studentSurveyMatrix
View(round(studentSurveyMatrix, 2)) # view the matrix
cortest.bartlett(studentSurvey1edit)
det(studentSurveyMatrix)
pcModel1 <- principal(studentSurvey1edit, nfactors = 10, rotate = "none")
pcModel1
plot(pcModel1$values, type="b")
pcModel2 <- principal(studentSurvey1edit, nfactors = 1, rotate = "none")
pcModel2
residuals <- factor.residuals(studentSurveyMatrix, pcModel2$loadings)
residuals <- as.matrix(residuals[upper.tri(residuals)])
largeResid <- abs(residuals) > .05
sum(largeResid)
sum(largeResid/nrow(residuals))
pcModel3 <- principal(studentSurvey1edit, nfactors = 1, rotate = "oblimin")
pcModel3
print.psych(pcModel3, cut = .3, sort=TRUE)
pcModel4 <- principal(studentSurvey1edit, nfactors = 1, rotate = "varimax")
print.psych(pcModel4, cut=.3, sort=TRUE)
View(financialWB)
studentSurvey = read.csv("data/studentSurvey.csv")
View(studentSurvey)
studentSurvey1 <- studentSurvey[, 31:42]
studentSurvey1
studentSurvey1edit <- drop_na(studentSurvey1)
View(studentSurvey1edit)
alpha(studentSurvey1edit)
pcModel3 <- principal(studentSurvey1edit, nfactors = 1, rotate = "oblimin")
pcModel3
alpha(studentSurvey1edit)
